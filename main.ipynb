{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5504,"status":"ok","timestamp":1679769741812,"user":{"displayName":"Rodion Khvorostov","userId":"14168350354996550512"},"user_tz":-60},"id":"a5wk81SB3LCK","outputId":"6100413b-fa7f-4adc-cd08-c02d8b4a42d7"},"outputs":[],"source":["# import pip\n","# import site\n","\n","# libs = ['transformers', 'datasets']\n","# for lib in libs:\n","#     pip.main(['install', lib])\n","# importlib.reload(site)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import transformers\n","import torch"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["'OnServer: False'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","onServer = True\n","if os.path.exists('config.py'):\n","    onServer = False\n","\n","f'OnServer: {onServer}'"]},{"cell_type":"code","execution_count":153,"metadata":{},"outputs":[],"source":["import os\n","os.environ[\"HF_HOME\"] = \"/output/cache\" if onServer else '/home/coder/project/cache'\n","os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"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\"\n"]},{"cell_type":"code","execution_count":154,"metadata":{},"outputs":[{"data":{"text/plain":["'/home/coder/project/cache'"]},"execution_count":154,"metadata":{},"output_type":"execute_result"}],"source":["os.getenv('HF_HOME')"]},{"cell_type":"code","execution_count":157,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n","Token is valid.\n","Your token has been saved to /home/coder/.cache/huggingface/token\n","Login successful\n"]}],"source":["from huggingface_hub import login, notebook_login\n","login(\"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\")\n","notebook_login()"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":925,"status":"ok","timestamp":1679769742722,"user":{"displayName":"Rodion Khvorostov","userId":"14168350354996550512"},"user_tz":-60},"id":"YUxTwMvU3Gdr","outputId":"47c588b1-877d-4ac1-9b59-6aa33d26a57d"},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading readme:   0%|          | 0.00/493 [00:00<?, ?B/s]\rDownloading readme: 100%|██████████| 493/493 [00:00<00:00, 328kB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Downloading and preparing dataset None/None to /home/coder/project/datasets/under-tree___parquet/under-tree--labeled-multiple-choice-8214d50786758969/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"]},{"name":"stderr","output_type":"stream","text":["Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\n","\rDownloading data:   0%|          | 0.00/1.29M [00:00<?, ?B/s]\u001b[A\rDownloading data: 100%|██████████| 1.29M/1.29M [00:00<00:00, 77.1MB/s]\n","\rDownloading data files: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it]\rDownloading data files: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it]\n","\rExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]\rExtracting data files: 100%|██████████| 1/1 [00:00<00:00, 926.51it/s]\n","\rGenerating train split:   0%|          | 0/36503 [00:00<?, ? examples/s]\r                                                                        "]},{"name":"stdout","output_type":"stream","text":["Dataset parquet downloaded and prepared to /home/coder/project/datasets/under-tree___parquet/under-tree--labeled-multiple-choice-8214d50786758969/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"]},{"name":"stderr","output_type":"stream","text":["\r"]}],"source":["from datasets import load_dataset\n","data = load_dataset('under-tree/labeled-multiple-choice', split='train')"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'answerKey': 'f',\n"," 'combinedfact': 'beads of water can be formed by clouds.',\n"," 'formatted_question': 'what type of water formation is formed by clouds? (a) '\n","                       'pearls (b) streams (c) shells (d) diamonds (e) rain '\n","                       '(f) beads (g) cooled (h) liquid',\n"," 'topic': 'physics'}\n"]}],"source":["from pprint import pprint\n","pprint(data[0])"]},{"cell_type":"code","execution_count":139,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1679769742723,"user":{"displayName":"Rodion Khvorostov","userId":"14168350354996550512"},"user_tz":-60},"id":"xBL7_fga3Gdw","outputId":"4aeb9270-5c46-4460-e448-4df7e2b299b5"},"outputs":[{"name":"stdout","output_type":"stream","text":["topic: anatomy\n","question: what requires rest? \n","variants: (a) kinetic energy (b) amphibians (c) all animals (d) watching tv (e) oxygen (f) most organisms (g) time and energy (h) cell regeneration\n","answer: h\n","context: cell regeneration requires rest\n","\n"]}],"source":["import numpy as np\n","n = np.random.randint(0, len(data))\n","def gen_prompt(elem):\n","    # return f'question: {elem.formatted_question}\\nanswer: {elem.answerKey}\\ncontext: {elem.combinedfact}\\n'\n","    # dict \n","    question, variants = elem['formatted_question'].split('(a)', 1)\n","    return {'text': f'topic: {elem[\"topic\"]}\\nquestion: {question}\\nvariants: (a){variants}\\nanswer: {elem[\"answerKey\"]}\\ncontext: {elem[\"combinedfact\"]}\\n'}\n","\n","print(gen_prompt(data[n])['text'])"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1679769742723,"user":{"displayName":"Rodion Khvorostov","userId":"14168350354996550512"},"user_tz":-60},"id":"J_WR3_El3Gdx","outputId":"8efc4ca7-e1de-46a0-dcd0-27f985ceab9c"},"outputs":[{"name":"stderr","output_type":"stream","text":["Map (num_proc=4):   0%|          | 0/36503 [00:00<?, ? examples/s]\rMap (num_proc=4):   1%|          | 451/36503 [00:00<00:10, 3334.26 examples/s]\rMap (num_proc=4):  10%|█         | 3776/36503 [00:00<00:01, 18223.73 examples/s]\rMap (num_proc=4):  22%|██▏       | 7909/36503 [00:00<00:01, 26073.67 examples/s]\rMap (num_proc=4):  33%|███▎      | 12138/36503 [00:00<00:00, 29042.71 examples/s]\rMap (num_proc=4):  44%|████▍     | 16103/36503 [00:00<00:00, 30411.05 examples/s]\rMap (num_proc=4):  57%|█████▋    | 20765/36503 [00:00<00:00, 34936.87 examples/s]\rMap (num_proc=4):  67%|██████▋   | 24448/36503 [00:00<00:00, 34761.87 examples/s]\rMap (num_proc=4):  77%|███████▋  | 28049/36503 [00:00<00:00, 35033.93 examples/s]\rMap (num_proc=4):  89%|████████▉ | 32491/36503 [00:01<00:00, 36549.94 examples/s]\rMap (num_proc=4): 100%|██████████| 36503/36503 [00:01<00:00, 33061.79 examples/s]\r                                                                                 \r"]}],"source":["data_with_prompt = data.map(gen_prompt, batched=False, remove_columns=data.column_names, num_proc=4)"]},{"cell_type":"code","execution_count":62,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3259,"status":"ok","timestamp":1679769745976,"user":{"displayName":"Rodion Khvorostov","userId":"14168350354996550512"},"user_tz":-60},"id":"KIImvYJY3Gdy","outputId":"54864c10-5fb6-486f-bab6-3c8dece3aeff"},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"data":{"text/plain":["Embedding(50263, 768)"]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","checkpoint = 'distilgpt2'\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint, pad_token='<|pad|>', use_fast=True)\n","special_tokens = {'additional_special_tokens': ['topic: ', 'question: ', 'variants: ', 'answer: ', 'context: ']}\n","tokenizer.add_special_tokens(special_tokens)\n","\n","model = AutoModelForCausalLM.from_pretrained(checkpoint)\n","model.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"code","execution_count":63,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1679769745976,"user":{"displayName":"Rodion Khvorostov","userId":"14168350354996550512"},"user_tz":-60},"id":"SJWHv5Vm3Gdy","outputId":"7728867c-171e-4203-8d4a-7c3e806f72c6"},"outputs":[{"data":{"text/plain":["{'input_ids': tensor([[50258,   746, 23154,   198, 50259, 10919,  2099,   286,  1660,  9978,\n","           318,  7042,   416, 15114,    30,   220,   198, 50260,     7,    64,\n","             8, 25286,  7278,   357,    65,     8, 15190,   357,    66,     8,\n","         19679,   357,    67,     8, 30984,   357,    68,     8,  6290,   357,\n","            69,     8, 36116,   357,    70,     8, 32162,   357,    71,     8,\n","          8122,   198, 50261,    69,   198, 50262,    65,  1329,    82,   286,\n","          1660,   460,   307,  7042,   416, 15114,    13,   198]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"]},"execution_count":63,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer(data_with_prompt[0]['text'], return_tensors='pt')"]},{"cell_type":"code","execution_count":64,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1679769745977,"user":{"displayName":"Rodion Khvorostov","userId":"14168350354996550512"},"user_tz":-60},"id":"INYCseTK3Gdz","outputId":"a5052315-72e4-426c-df4c-19bcb8494c7c"},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading cached processed dataset at /home/coder/project/datasets/under-tree___parquet/under-tree--labeled-multiple-choice-8214d50786758969/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-7ba3eded628cbdf0_*_of_00004.arrow\n"]},{"name":"stdout","output_type":"stream","text":["Max:  766\n"]}],"source":["def encode(elem):\n","    return tokenizer(elem['text'], truncation=True)\n","\n","data_encoded = data_with_prompt.map(encode, batched=True, remove_columns=data_with_prompt.column_names, num_proc=4)"]},{"cell_type":"code","execution_count":65,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1679769745977,"user":{"displayName":"Rodion Khvorostov","userId":"14168350354996550512"},"user_tz":-60},"id":"GLWnTY9q3Gdz"},"outputs":[],"source":["block_size = 128\n","\n","def group_texts(examples):\n","    # Concatenate all texts.\n","    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n","    total_length = len(concatenated_examples[list(examples.keys())[0]])\n","    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n","        # customize this part to your needs.\n","    total_length = (total_length // block_size) * block_size\n","    # Split by chunks of max_len.\n","    result = {\n","        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n","        for k, t in concatenated_examples.items()\n","    }\n","    result[\"labels\"] = result[\"input_ids\"].copy()\n","    return result"]},{"cell_type":"code","execution_count":66,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1679769745978,"user":{"displayName":"Rodion Khvorostov","userId":"14168350354996550512"},"user_tz":-60},"id":"X_dzpuU53Gd0","outputId":"648f5fdb-41f0-4ba1-be3f-a8c44d5b4133"},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading cached processed dataset at /home/coder/project/datasets/under-tree___parquet/under-tree--labeled-multiple-choice-8214d50786758969/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-57c83507ae2ffb2d_*_of_00004.arrow\n"]}],"source":["data_lm = data_encoded.map(group_texts, batched=True, num_proc=4)"]},{"cell_type":"code","execution_count":67,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1679769745978,"user":{"displayName":"Rodion Khvorostov","userId":"14168350354996550512"},"user_tz":-60},"id":"IYB43mhr3Gd0"},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading cached split indices for dataset at /home/coder/project/datasets/under-tree___parquet/under-tree--labeled-multiple-choice-8214d50786758969/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-43d7296b5d347e17.arrow and /home/coder/project/datasets/under-tree___parquet/under-tree--labeled-multiple-choice-8214d50786758969/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-a86ee990c2f3a6e1.arrow\n"]}],"source":["data_dict = data_lm.train_test_split(test_size=0.2)"]},{"cell_type":"code","execution_count":170,"metadata":{},"outputs":[],"source":["# del training_args\n","# del trainer"]},{"cell_type":"code","execution_count":171,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":300,"status":"ok","timestamp":1679769746268,"user":{"displayName":"Rodion Khvorostov","userId":"14168350354996550512"},"user_tz":-60},"id":"4uADL10h3Gd1","outputId":"9cc9ea47-a8bf-4d3c-8e4e-b25242a89966"},"outputs":[],"source":["from transformers import Trainer, TrainingArguments\n","\n","batch_size_device = 40\n","training_args = TrainingArguments(\n","    'choice-question-generator',   \n","    evaluation_strategy='epoch',\n","    num_train_epochs=3,\n","    per_device_train_batch_size=batch_size_device,\n","    per_device_eval_batch_size=batch_size_device,\n","    push_to_hub=True\n",")"]},{"cell_type":"code","execution_count":172,"metadata":{"executionInfo":{"elapsed":203,"status":"ok","timestamp":1679769884054,"user":{"displayName":"Rodion Khvorostov","userId":"14168350354996550512"},"user_tz":-60},"id":"vSmuoDfL3Gd1"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/coder/project/tmp/choice-question-generator is already a clone of https://huggingface.co/under-tree/choice-question-generator. Make sure you pull the latest changes with `repo.git_pull()`.\n"]}],"source":["# default args are pretty good: https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=data_dict['train'],\n","    eval_dataset=data_dict['test']\n",")"]},{"cell_type":"markdown","metadata":{},"source":["I tried different ways to accelerate the training\n","\n","1. PyTorch with devices\n","2. PyTorch with accelerator\n","3. Default Trainer with accelerator\n","4. Default Trainer\n","5. Trainer with change of batch_size, fp16=True\n","\n","The **last method is fastest**"]},{"cell_type":"code","execution_count":161,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":130},"id":"QhxNB4F_3Gd1","outputId":"9cdd92cf-88df-4c14-8a39-0745433b8694"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/coder/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='636' max='636' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [636/636 04:41, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>1.086173</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>1.063922</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>1.095000</td>\n","      <td>1.055819</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/plain":["TrainOutput(global_step=636, training_loss=1.0883979557445214, metrics={'train_runtime': 282.3028, 'train_samples_per_second': 180.115, 'train_steps_per_second': 2.253, 'total_flos': 1660769484668928.0, 'train_loss': 1.0883979557445214, 'epoch': 3.0})"]},"execution_count":161,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()\n","# trainer.save_model('result/')"]},{"cell_type":"code","execution_count":162,"metadata":{},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='53' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [53/53 00:09]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Perplexity: 2.87\n"]}],"source":["eval_results = trainer.evaluate() # returns cross entropy\n","print(f\"Perplexity: {np.exp(eval_results['eval_loss']):.2f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":174,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/coder/project/tmp/choice-question-generator is already a clone of https://huggingface.co/under-tree/choice-question-generator. Make sure you pull the latest changes with `repo.git_pull()`.\n"]}],"source":["from huggingface_hub import Repository\n","\n","modelname = 'choice-question-generator'\n","username = 'under-tree'\n","repo = Repository(modelname, clone_from=f\"{username}/{modelname}\")"]},{"cell_type":"code","execution_count":168,"metadata":{},"outputs":[],"source":["trainer.push_to_hub()"]},{"cell_type":"code","execution_count":167,"metadata":{},"outputs":[{"ename":"OSError","evalue":"under-tree/choice-question-generator does not appear to have a file named config.json. Checkout 'https://huggingface.co/under-tree/choice-question-generator/main' for available files.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","File \u001b[0;32m~/.local/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py:259\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 259\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    260\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n","File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n","\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/under-tree/choice-question-generator/resolve/main/config.json","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mEntryNotFoundError\u001b[0m                        Traceback (most recent call last)","File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/utils/hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    410\u001b[0m         path_or_repo_id,\n\u001b[1;32m    411\u001b[0m         filename,\n\u001b[1;32m    412\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    413\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    414\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    415\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    416\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    417\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    418\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    419\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    420\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    421\u001b[0m     )\n\u001b[1;32m    423\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:120\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 120\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/huggingface_hub/file_download.py:1160\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1159\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1160\u001b[0m     metadata \u001b[39m=\u001b[39m get_hf_file_metadata(\n\u001b[1;32m   1161\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1162\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   1163\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1164\u001b[0m         timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[1;32m   1165\u001b[0m     )\n\u001b[1;32m   1166\u001b[0m \u001b[39mexcept\u001b[39;00m EntryNotFoundError \u001b[39mas\u001b[39;00m http_error:\n\u001b[1;32m   1167\u001b[0m     \u001b[39m# Cache the non-existence of the file and raise\u001b[39;00m\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:120\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 120\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/huggingface_hub/file_download.py:1501\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1492\u001b[0m r \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1493\u001b[0m     method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mHEAD\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1494\u001b[0m     url\u001b[39m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1499\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[1;32m   1500\u001b[0m )\n\u001b[0;32m-> 1501\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1503\u001b[0m \u001b[39m# Return\u001b[39;00m\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py:269\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    268\u001b[0m     message \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m Client Error.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEntry Not Found for url: \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39murl\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 269\u001b[0m     \u001b[39mraise\u001b[39;00m EntryNotFoundError(message, response) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[39melif\u001b[39;00m error_code \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mGatedRepo\u001b[39m\u001b[39m\"\u001b[39m:\n","\u001b[0;31mEntryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-6420942c-0fd46b16613c421e2c51eb5b)\n\nEntry Not Found for url: https://huggingface.co/under-tree/choice-question-generator/resolve/main/config.json.","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[0;32mIn[167], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n\u001b[0;32m----> 3\u001b[0m generator \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39;49m\u001b[39mtext-generation\u001b[39;49m\u001b[39m\"\u001b[39;49m, model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39munder-tree/choice-question-generator\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m generator(\n\u001b[1;32m      5\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtopic: chemistry\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     num_return_sequences\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,\n\u001b[1;32m      7\u001b[0m )\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/pipelines/__init__.py:692\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m     hub_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39m_commit_hash\n\u001b[1;32m    691\u001b[0m \u001b[39melif\u001b[39;00m config \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(model, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 692\u001b[0m     config \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39;49mfrom_pretrained(model, _from_pipeline\u001b[39m=\u001b[39;49mtask, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m    693\u001b[0m     hub_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39m_commit_hash\n\u001b[1;32m    695\u001b[0m custom_tasks \u001b[39m=\u001b[39m {}\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py:896\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    894\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mname_or_path\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m pretrained_model_name_or_path\n\u001b[1;32m    895\u001b[0m trust_remote_code \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtrust_remote_code\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 896\u001b[0m config_dict, unused_kwargs \u001b[39m=\u001b[39m PretrainedConfig\u001b[39m.\u001b[39;49mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    897\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mAutoConfig\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    898\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m trust_remote_code:\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:573\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    572\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 573\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    574\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n\u001b[1;32m    575\u001b[0m     original_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config_dict[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:628\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    624\u001b[0m configuration_file \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39m_configuration_file\u001b[39m\u001b[39m\"\u001b[39m, CONFIG_NAME)\n\u001b[1;32m    626\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m     \u001b[39m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    629\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    630\u001b[0m         configuration_file,\n\u001b[1;32m    631\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    632\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    633\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    634\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    635\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    636\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    637\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    638\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    639\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    640\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m    641\u001b[0m     )\n\u001b[1;32m    642\u001b[0m     commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    643\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    644\u001b[0m     \u001b[39m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    645\u001b[0m     \u001b[39m# the original exception.\u001b[39;00m\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/utils/hub.py:454\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    452\u001b[0m     \u001b[39mif\u001b[39;00m revision \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m         revision \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmain\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 454\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    455\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m does not appear to have a file named \u001b[39m\u001b[39m{\u001b[39;00mfull_filename\u001b[39m}\u001b[39;00m\u001b[39m. Checkout \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    456\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mrevision\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m for available files.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    457\u001b[0m     )\n\u001b[1;32m    458\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    459\u001b[0m     \u001b[39m# First we try to see if we have a cached version (not up to date):\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     resolved_file \u001b[39m=\u001b[39m try_to_load_from_cache(path_or_repo_id, full_filename, cache_dir\u001b[39m=\u001b[39mcache_dir, revision\u001b[39m=\u001b[39mrevision)\n","\u001b[0;31mOSError\u001b[0m: under-tree/choice-question-generator does not appear to have a file named config.json. Checkout 'https://huggingface.co/under-tree/choice-question-generator/main' for available files."]}],"source":["from transformers import pipeline\n","\n","generator = pipeline(\"text-generation\", model=\"under-tree/choice-question-generator\")\n","generator(\n","    \"topic: chemistry\",\n","    num_return_sequences=3,\n",")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"}}},"nbformat":4,"nbformat_minor":0}
