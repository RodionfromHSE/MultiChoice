{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5504,"status":"ok","timestamp":1679769741812,"user":{"displayName":"Rodion Khvorostov","userId":"14168350354996550512"},"user_tz":-60},"id":"a5wk81SB3LCK","outputId":"6100413b-fa7f-4adc-cd08-c02d8b4a42d7"},"outputs":[],"source":["# import pip\n","# import site\n","\n","# libs = ['transformers', 'datasets']\n","# for lib in libs:\n","#     pip.main(['install', lib])\n","# importlib.reload(site)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import transformers\n","import torch"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["'OnServer: False'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","onServer = True\n","if os.path.exists('config.py'):\n","    onServer = False\n","\n","f'OnServer: {onServer}'"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["import os\n","os.environ[\"HF_HOME\"] = \"/output/\" if onServer else '/home/coder/project/'"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":925,"status":"ok","timestamp":1679769742722,"user":{"displayName":"Rodion Khvorostov","userId":"14168350354996550512"},"user_tz":-60},"id":"YUxTwMvU3Gdr","outputId":"47c588b1-877d-4ac1-9b59-6aa33d26a57d"},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading readme:   0%|          | 0.00/493 [00:00<?, ?B/s]\rDownloading readme: 100%|██████████| 493/493 [00:00<00:00, 328kB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Downloading and preparing dataset None/None to /home/coder/project/datasets/under-tree___parquet/under-tree--labeled-multiple-choice-8214d50786758969/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"]},{"name":"stderr","output_type":"stream","text":["Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\n","\rDownloading data:   0%|          | 0.00/1.29M [00:00<?, ?B/s]\u001b[A\rDownloading data: 100%|██████████| 1.29M/1.29M [00:00<00:00, 77.1MB/s]\n","\rDownloading data files: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it]\rDownloading data files: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it]\n","\rExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]\rExtracting data files: 100%|██████████| 1/1 [00:00<00:00, 926.51it/s]\n","\rGenerating train split:   0%|          | 0/36503 [00:00<?, ? examples/s]\r                                                                        "]},{"name":"stdout","output_type":"stream","text":["Dataset parquet downloaded and prepared to /home/coder/project/datasets/under-tree___parquet/under-tree--labeled-multiple-choice-8214d50786758969/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"]},{"name":"stderr","output_type":"stream","text":["\r"]}],"source":["from datasets import load_dataset\n","data = load_dataset('under-tree/labeled-multiple-choice', split='train')"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'answerKey': 'f',\n"," 'combinedfact': 'beads of water can be formed by clouds.',\n"," 'formatted_question': 'what type of water formation is formed by clouds? (a) '\n","                       'pearls (b) streams (c) shells (d) diamonds (e) rain '\n","                       '(f) beads (g) cooled (h) liquid',\n"," 'topic': 'physics'}\n"]}],"source":["from pprint import pprint\n","pprint(data[0])"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1679769742723,"user":{"displayName":"Rodion Khvorostov","userId":"14168350354996550512"},"user_tz":-60},"id":"xBL7_fga3Gdw","outputId":"4aeb9270-5c46-4460-e448-4df7e2b299b5"},"outputs":[{"name":"stdout","output_type":"stream","text":["topic: engineering\n","question: what kind of beads are formed from vapor condensing? \n","variants: (a) tiny (b) h20 (c) h2o (d) carbon (e) hydrogen (f) rain (g) oxygen (h) dew\n","answer: c\n","context: h2o beads are formed by h2o vapor condensing\n","\n"]}],"source":["def gen_prompt(elem):\n","    # return f'question: {elem.formatted_question}\\nanswer: {elem.answerKey}\\ncontext: {elem.combinedfact}\\n'\n","    # dict \n","    question, variants = elem['formatted_question'].split('(a)', 1)\n","    return {'text': f'topic: {elem[\"topic\"]}\\nquestion: {question}\\nvariants: (a){variants}\\nanswer: {elem[\"answerKey\"]}\\ncontext: {elem[\"combinedfact\"]}\\n'}\n","print(gen_prompt(data[13])['text'])"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1679769742723,"user":{"displayName":"Rodion Khvorostov","userId":"14168350354996550512"},"user_tz":-60},"id":"J_WR3_El3Gdx","outputId":"8efc4ca7-e1de-46a0-dcd0-27f985ceab9c"},"outputs":[{"name":"stderr","output_type":"stream","text":["Map (num_proc=4):   0%|          | 0/36503 [00:00<?, ? examples/s]\rMap (num_proc=4):   1%|          | 451/36503 [00:00<00:10, 3334.26 examples/s]\rMap (num_proc=4):  10%|█         | 3776/36503 [00:00<00:01, 18223.73 examples/s]\rMap (num_proc=4):  22%|██▏       | 7909/36503 [00:00<00:01, 26073.67 examples/s]\rMap (num_proc=4):  33%|███▎      | 12138/36503 [00:00<00:00, 29042.71 examples/s]\rMap (num_proc=4):  44%|████▍     | 16103/36503 [00:00<00:00, 30411.05 examples/s]\rMap (num_proc=4):  57%|█████▋    | 20765/36503 [00:00<00:00, 34936.87 examples/s]\rMap (num_proc=4):  67%|██████▋   | 24448/36503 [00:00<00:00, 34761.87 examples/s]\rMap (num_proc=4):  77%|███████▋  | 28049/36503 [00:00<00:00, 35033.93 examples/s]\rMap (num_proc=4):  89%|████████▉ | 32491/36503 [00:01<00:00, 36549.94 examples/s]\rMap (num_proc=4): 100%|██████████| 36503/36503 [00:01<00:00, 33061.79 examples/s]\r                                                                                 \r"]}],"source":["data_with_prompt = data.map(gen_prompt, batched=False, remove_columns=data.column_names, num_proc=4)"]},{"cell_type":"code","execution_count":62,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3259,"status":"ok","timestamp":1679769745976,"user":{"displayName":"Rodion Khvorostov","userId":"14168350354996550512"},"user_tz":-60},"id":"KIImvYJY3Gdy","outputId":"54864c10-5fb6-486f-bab6-3c8dece3aeff"},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"data":{"text/plain":["Embedding(50263, 768)"]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","checkpoint = 'distilgpt2'\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint, pad_token='<|pad|>', use_fast=True)\n","special_tokens = {'additional_special_tokens': ['topic: ', 'question: ', 'variants: ', 'answer: ', 'context: ']}\n","tokenizer.add_special_tokens(special_tokens)\n","\n","model = AutoModelForCausalLM.from_pretrained(checkpoint)\n","model.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"code","execution_count":63,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1679769745976,"user":{"displayName":"Rodion Khvorostov","userId":"14168350354996550512"},"user_tz":-60},"id":"SJWHv5Vm3Gdy","outputId":"7728867c-171e-4203-8d4a-7c3e806f72c6"},"outputs":[{"data":{"text/plain":["{'input_ids': tensor([[50258,   746, 23154,   198, 50259, 10919,  2099,   286,  1660,  9978,\n","           318,  7042,   416, 15114,    30,   220,   198, 50260,     7,    64,\n","             8, 25286,  7278,   357,    65,     8, 15190,   357,    66,     8,\n","         19679,   357,    67,     8, 30984,   357,    68,     8,  6290,   357,\n","            69,     8, 36116,   357,    70,     8, 32162,   357,    71,     8,\n","          8122,   198, 50261,    69,   198, 50262,    65,  1329,    82,   286,\n","          1660,   460,   307,  7042,   416, 15114,    13,   198]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"]},"execution_count":63,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer(data_with_prompt[0]['text'], return_tensors='pt')"]},{"cell_type":"code","execution_count":64,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1679769745977,"user":{"displayName":"Rodion Khvorostov","userId":"14168350354996550512"},"user_tz":-60},"id":"INYCseTK3Gdz","outputId":"a5052315-72e4-426c-df4c-19bcb8494c7c"},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading cached processed dataset at /home/coder/project/datasets/under-tree___parquet/under-tree--labeled-multiple-choice-8214d50786758969/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-7ba3eded628cbdf0_*_of_00004.arrow\n"]},{"name":"stdout","output_type":"stream","text":["Max:  766\n"]}],"source":["mx = max(map(len, data_with_prompt['text']))\n","print('Max: ', mx)\n","\n","def encode(elem):\n","    return tokenizer(elem['text'], truncation=True)\n","\n","data_encoded = data_with_prompt.map(encode, batched=True, remove_columns=data_with_prompt.column_names, num_proc=4)"]},{"cell_type":"code","execution_count":65,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1679769745977,"user":{"displayName":"Rodion Khvorostov","userId":"14168350354996550512"},"user_tz":-60},"id":"GLWnTY9q3Gdz"},"outputs":[],"source":["block_size = 128\n","\n","def group_texts(examples):\n","    # Concatenate all texts.\n","    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n","    total_length = len(concatenated_examples[list(examples.keys())[0]])\n","    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n","        # customize this part to your needs.\n","    total_length = (total_length // block_size) * block_size\n","    # Split by chunks of max_len.\n","    result = {\n","        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n","        for k, t in concatenated_examples.items()\n","    }\n","    result[\"labels\"] = result[\"input_ids\"].copy()\n","    return result"]},{"cell_type":"code","execution_count":66,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1679769745978,"user":{"displayName":"Rodion Khvorostov","userId":"14168350354996550512"},"user_tz":-60},"id":"X_dzpuU53Gd0","outputId":"648f5fdb-41f0-4ba1-be3f-a8c44d5b4133"},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading cached processed dataset at /home/coder/project/datasets/under-tree___parquet/under-tree--labeled-multiple-choice-8214d50786758969/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-57c83507ae2ffb2d_*_of_00004.arrow\n"]}],"source":["data_lm = data_encoded.map(group_texts, batched=True, num_proc=4)"]},{"cell_type":"code","execution_count":67,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1679769745978,"user":{"displayName":"Rodion Khvorostov","userId":"14168350354996550512"},"user_tz":-60},"id":"IYB43mhr3Gd0"},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading cached split indices for dataset at /home/coder/project/datasets/under-tree___parquet/under-tree--labeled-multiple-choice-8214d50786758969/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-43d7296b5d347e17.arrow and /home/coder/project/datasets/under-tree___parquet/under-tree--labeled-multiple-choice-8214d50786758969/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-a86ee990c2f3a6e1.arrow\n"]}],"source":["data_dict = data_lm.train_test_split(test_size=0.2)"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[],"source":["# del training_args\n","# del trainer"]},{"cell_type":"code","execution_count":75,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":300,"status":"ok","timestamp":1679769746268,"user":{"displayName":"Rodion Khvorostov","userId":"14168350354996550512"},"user_tz":-60},"id":"4uADL10h3Gd1","outputId":"9cc9ea47-a8bf-4d3c-8e4e-b25242a89966"},"outputs":[],"source":["from transformers import Trainer, TrainingArguments\n","\n","batch_size_device = 40\n","training_args = TrainingArguments(\n","    output_dir='./rodya',   \n","    evaluation_strategy='epoch',\n","    num_train_epochs=3,\n","    per_device_train_batch_size=batch_size_device,\n","    per_device_eval_batch_size=batch_size_device\n",")"]},{"cell_type":"code","execution_count":76,"metadata":{"executionInfo":{"elapsed":203,"status":"ok","timestamp":1679769884054,"user":{"displayName":"Rodion Khvorostov","userId":"14168350354996550512"},"user_tz":-60},"id":"vSmuoDfL3Gd1"},"outputs":[],"source":["# default args are pretty good: https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=data_dict['train'],\n","    eval_dataset=data_dict['test']\n",")"]},{"cell_type":"markdown","metadata":{},"source":["I tried different ways to accelerate the training\n","\n","1. PyTorch with devices\n","2. PyTorch with accelerator\n","3. Default Trainer with accelerator\n","4. Default Trainer\n","5. Trainer with change of batch_size, fp16=True\n","\n","The **last method is fastest**"]},{"cell_type":"code","execution_count":77,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":130},"id":"QhxNB4F_3Gd1","outputId":"9cdd92cf-88df-4c14-8a39-0745433b8694"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/coder/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='636' max='636' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [636/636 04:10, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>1.182352</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>1.138813</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>1.225400</td>\n","      <td>1.126801</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]}],"source":["trainer.train()\n","trainer.save_model('result/')"]},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='53' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [53/53 00:08]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Perplexity: 3.09\n"]}],"source":["import math\n","eval_results = trainer.evaluate()\n","print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"]},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3b56f1a4051343c6aad3116817c82fb1","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"]},"metadata":{},"output_type":"display_data"}],"source":["from huggingface_hub import login\n","login()"]},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[{"data":{"text/plain":["{'input_ids': [50258, 746, 23154], 'attention_mask': [1, 1, 1]}"]},"execution_count":83,"metadata":{},"output_type":"execute_result"}],"source":["tokens"]},{"cell_type":"code","execution_count":90,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\r\n","Requirement already satisfied: huggingface_hub in /home/coder/.local/lib/python3.8/site-packages (0.13.3)\r\n","Requirement already satisfied: packaging>=20.9 in /home/coder/.local/lib/python3.8/site-packages (from huggingface_hub) (23.0)\r\n","Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.8/site-packages (from huggingface_hub) (4.65.0)\r\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface_hub) (4.4.0)\r\n","Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from huggingface_hub) (2.28.2)\r\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from huggingface_hub) (3.9.0)\r\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from huggingface_hub) (6.0)\r\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->huggingface_hub) (2022.12.7)\r\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests->huggingface_hub) (2.1.1)\r\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->huggingface_hub) (1.26.14)\r\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->huggingface_hub) (3.4)\r\n"]}],"source":["!pip install huggingface_hub"]},{"cell_type":"code","execution_count":91,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["zsh:1: command not found: huggingface-cli\n"]}],"source":["!huggingface-cli login"]},{"cell_type":"code","execution_count":104,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n","Token is valid.\n","Your token has been saved to /home/coder/.cache/huggingface/token\n","Login successful\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7971741b0b374e1e9ae73a0797492d88","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"]},"metadata":{},"output_type":"display_data"}],"source":["from huggingface_hub import notebook_login, login\n","login(\"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\")\n","notebook_login()"]},{"cell_type":"code","execution_count":88,"metadata":{},"outputs":[{"ename":"OSError","evalue":"Can't load tokenizer for 'result/'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'result/' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[0;32mIn[88], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoModelForSequenceClassification, AutoTokenizer\n\u001b[1;32m      3\u001b[0m \u001b[39m# Load the tokenizer and model from the saved checkpoint\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39mresult/\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      5\u001b[0m model \u001b[39m=\u001b[39m AutoModelForSequenceClassification\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mresult/\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[39m# Define the input text for which you want to generate a prediction\u001b[39;00m\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:697\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    695\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[39m=\u001b[39m TOKENIZER_MAPPING[\u001b[39mtype\u001b[39m(config)]\n\u001b[1;32m    696\u001b[0m \u001b[39mif\u001b[39;00m tokenizer_class_fast \u001b[39mand\u001b[39;00m (use_fast \u001b[39mor\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 697\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class_fast\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1788\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m   1783\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load following files from cache: \u001b[39m\u001b[39m{\u001b[39;00munresolved_files\u001b[39m}\u001b[39;00m\u001b[39m and cannot check if these \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1784\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfiles are necessary for the tokenizer to operate.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1785\u001b[0m     )\n\u001b[1;32m   1787\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(full_file_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m full_file_name \u001b[39min\u001b[39;00m resolved_vocab_files\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m-> 1788\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   1789\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load tokenizer for \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. If you were trying to load it from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1790\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, make sure you don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt have a local directory with the same name. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1791\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOtherwise, make sure \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is the correct path to a directory \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1792\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcontaining all relevant files for a \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m tokenizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1793\u001b[0m     )\n\u001b[1;32m   1795\u001b[0m \u001b[39mfor\u001b[39;00m file_id, file_path \u001b[39min\u001b[39;00m vocab_files\u001b[39m.\u001b[39mitems():\n\u001b[1;32m   1796\u001b[0m     \u001b[39mif\u001b[39;00m file_id \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m resolved_vocab_files:\n","\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'result/'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'result/' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer."]}],"source":["from transformers import AutoModelForSequenceClassification, AutoTokenizer\n","\n","# Load the tokenizer and model from the saved checkpoint\n","tokenizer = AutoTokenizer.from_pretrained(dir='result/')\n","model = AutoModelForSequenceClassification.from_pretrained(dir='result/')\n","\n","# Define the input text for which you want to generate a prediction\n","inputs = \"topic: physics\"\n","\n","# Tokenize the input text\n","tokens = tokenizer(inputs, padding=True, truncation=True, return_tensors=\"pt\")\n","\n","# Use the model to generate predictions\n","outputs = model(**tokens)\n","predictions = outputs.logits.softmax(dim=-1).detach().numpy()\n","\n","# Print the predicted probabilities for each label\n","print(predictions)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16 | packaged by conda-forge | (default, Feb  1 2023, 16:01:55) \n[GCC 11.3.0]"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"}}},"nbformat":4,"nbformat_minor":0}
